using MDPs
import MDPs: preexperiment, preepisode, prestep, poststep, postepisode, postexperiment, VectorSpace
using Flux
using Random
using UnPack
using DataStructures
using StatsBase

export DQNLearner

mutable struct DQNLearner{T<:AbstractFloat} <: AbstractHook
    Ï€::DQNPolicy{T}
    Î³::Float32
    Ï::Float32
    min_explore_steps::Int
    train_interval::Int
    gradsteps::Int
    batch_size::Int

    s::Union{Vector{T}, Nothing}
    buff::CircularBuffer{Tuple{Vector{T}, Int, Float64, Vector{T}, Bool}}
    qmodelâ€²
    optim

    stats::Dict{Symbol, Float32}

    function DQNLearner(Ï€::DQNPolicy{T}, Î³, Î±; polyak=0.995, min_explore_steps=10000, train_interval=1, gradsteps=1, batch_size=32, buffer_size=1000000, clipnorm=Inf, clipval=Inf) where T <: AbstractFloat
        buff = CircularBuffer{Tuple{Vector{T}, Int, Float64, Vector{T}, Bool}}(buffer_size)
        optim = Adam(Î±)
        if clipnorm < Inf;
            optim = Flux.Optimiser(Flux.Optimise.ClipNorm(clipnorm), optim)
        else
            if clipval < Inf; optim = Flux.Optimiser(Flux.Optimise.ClipNorm(clipnorm), optim); end
        end
        new{T}(Ï€, Î³, polyak, min_explore_steps, train_interval, gradsteps, batch_size, nothing, buff, deepcopy(Ï€.qmodel), optim, Dict{Symbol, Float32}())
    end
end

function prestep(dqn::DQNLearner; env::AbstractMDP, kwargs...)
    dqn.s = copy(state(env))
end

function poststep(dqn::DQNLearner{T}; env::AbstractMDP{Vector{T}, Int}, steps::Int, rng::AbstractRNG, returns, kwargs...) where T <: AbstractFloat
    @unpack Ï€, Î³, Ï, batch_size, s, qmodelâ€² = dqn

    a, r, sâ€², d = action(env), reward(env), copy(state(env)), in_absorbing_state(env)
    push!(dqn.buff, (s, a, r, sâ€², d))

    if steps >= dqn.min_explore_steps && steps % dqn.train_interval == 0
        for gradstep in 1:dqn.gradsteps
            replay_batch = rand(rng, dqn.buff, batch_size)
            ğ¬, ğš, ğ«, ğ¬â€², ğ = map(i -> reduce((ğ±, y) -> cat(ğ±, y; dims=ndims(y) + 1), map(experience -> experience[i], replay_batch)), 1:5)
            ğ¬, ğ«, ğ¬â€², ğ = tof32.((ğ¬, ğ«, ğ¬â€², ğ))
            ğš_ğ¬ = map(j -> CartesianIndex(ğš[j], j), 1:batch_size)
            
            Î¸ = Flux.params(Ï€.qmodel)
            â„“, âˆ‡Î¸â„“ = Flux.Zygote.withgradient(Î¸) do
                ğªÌ‚ = Ï€.qmodel(ğ¬)
                ğª = Flux.Zygote.ignore() do
                    ğ›‘â€² = Ï€(ğ¬â€², :)
                    ğªâ€² = qmodelâ€²(ğ¬â€²)
                    ğ¯â€² = sum(ğ›‘â€² .* ğªâ€², dims=1)[1, :]
                    ğ›… = zeros(Float32, size(ğªÌ‚)) # TD error
                    ğ›…[ğš_ğ¬] = ğ« + (1 .- ğ) * Î³ .* ğ¯â€² - @view ğªÌ‚[ğš_ğ¬]
                    ğªÌ‚ + ğ›…
                end
                Flux.mse(ğªÌ‚, ğª)
            end

            Flux.update!(dqn.optim, Î¸, âˆ‡Î¸â„“)

            vÌ„ = mean(sum(Ï€(ğ¬, :) .* Ï€.qmodel(ğ¬), dims=1))
            dqn.stats[:vÌ„] = vÌ„
            dqn.stats[:â„“] = â„“
        end

        Î¸ = Flux.params(Ï€.qmodel)
        Î¸â€² = Flux.params(qmodelâ€²)
        Flux.loadparams!(qmodelâ€², Ï .* Î¸â€² .+ (1 - Ï) .* Î¸)

        if steps % 1000 == 0
            episodes = length(returns)
            @debug "learning stats" steps episodes dqn.stats...
        end
    end
    nothing
end
